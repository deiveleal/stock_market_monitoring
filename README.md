# Real-Time Data Pipeline for Stock Market Monitoring (Open Source)

## ğŸ“‹ Description

This project develops a real-time data pipeline for stock market monitoring using exclusively **open source** tools. The system captures stock market data such as prices, volumes, and other financial indicators, processes and stores the information, making it accessible for analysis and interactive visualization. This open-source approach ensures greater flexibility and accessibility.

---

## ğŸ› ï¸ Project Stages

### 1. **Data Collection**
- **Data Source**: Use free APIs like [Alpha Vantage](https://www.alphavantage.co/) or [Yahoo Finance API](https://finance.yahoo.com/).
- **Ingestion**: Set up real-time data ingestion with **Apache Kafka**.
- **Security**: Secure API keys using configuration files and manage request limits.

---

### 2. **Processing and Transformation**
- **Streaming**: Implement real-time data processing using **Apache Spark Streaming** or **Apache Flink**.
- **Data Cleaning**: Standardize formats, handle missing values, and correct inconsistencies.
- **Data Enrichment**: Compute technical indicators using Python libraries such as:
  - **TA-Lib** (Technical Analysis Library)
  - **Pandas** for data manipulation.

---

### 3. **Storage**
- **Real-Time Database**: Use **Apache Cassandra** or **PostgreSQL** with the **TimescaleDB** extension.
- **Data Lake**: Use **MinIO** as an open-source alternative for raw data storage.
- **Partitioning**: Structure data by time or symbol to optimize queries.

---

### 4. **Visualization and Analysis**
- **Dashboards**: Build interactive dashboards using **Apache Superset** or **Metabase**.
- **Alerts**: Configure automated notifications using **Apache NiFi** or **Prometheus Alertmanager**.
- **Reports**: Generate reports with Python libraries such as **Matplotlib** and **Seaborn** for custom visualizations.

---

## ğŸ› ï¸ Technologies and Tools

| **Category**            | **Technology/Tool**                                        |
|-------------------------|------------------------------------------------------------|
| Programming Languages    | Python (pandas, NumPy, TA-Lib), Scala                     |
| Streaming Frameworks     | Apache Kafka, Apache Spark Streaming, Apache Flink        |
| Databases                | Apache Cassandra, PostgreSQL (TimescaleDB)                |
| Visualization Tools      | Apache Superset, Metabase                                 |
| Data Lake                | MinIO                                                     |
| Other Tools              | Prometheus, Apache NiFi                                   |

---

## âš ï¸ Challenges and Considerations

- **Latency**: Ensure low latency throughout the pipeline.
- **Scalability**: Configure the system to handle increasing data volumes efficiently.
- **Reliability**: Implement fault-tolerance with backups and replication.

---

## ğŸ“Š Expected Outcomes

- **Continuous Monitoring**: Real-time financial data available for analysis.
- **Interactive Visualizations**: Customizable dashboards to display trends and patterns.
- **High-Quality Insights**: Clean and enriched data for deeper analysis.

---

## ğŸš€ How to Run

### Prerequisites
1. Install the required dependencies listed in `requirements.txt`.
2. Configure Kafka and Cassandra/PostgreSQL using the provided setup scripts.
3. Set environment variables for API keys and ingestion parameters.
4. Run the pipeline by following the commands in the main project directory.

---

## ğŸ¤ Contributions

Contributions are welcome! ğŸ‰ If you encounter any issues, have suggestions for improvements, or want to add new features, feel free to open an [issue](#) or submit a [pull request](#).

---

**PT-BR**
# Pipeline de Dados em Tempo Real para Monitoramento do Mercado de AÃ§Ãµes (Open Source)

## ğŸ“‹ DescriÃ§Ã£o

Este projeto desenvolve um pipeline de dados em tempo real para monitorar o mercado de aÃ§Ãµes, utilizando exclusivamente ferramentas **open source**. O sistema captura dados de preÃ§os, volumes e outros indicadores financeiros, processa e armazena as informaÃ§Ãµes, disponibilizando-as para anÃ¡lise e visualizaÃ§Ã£o interativa. Com esta abordagem, garantimos maior acessibilidade e flexibilidade na implementaÃ§Ã£o.

---

## ğŸ› ï¸ Etapas do Projeto

### 1. **Coleta de Dados**
- **Fonte de Dados**: Utilizar [Yahoo Finance API](https://finance.yahoo.com/).
- **IngestÃ£o**: Configurar ingestÃ£o de dados em tempo real com **Apache Kafka**.
- **SeguranÃ§a**: Proteger chaves de API com arquivos de configuraÃ§Ã£o e limitar o nÃºmero de requisiÃ§Ãµes.

---

### 2. **Processamento e TransformaÃ§Ã£o**
- **Streaming**: Implementar processamento em tempo real utilizando **Apache Spark Streaming** ou **Apache Flink**.
- **Limpeza de Dados**: Padronizar formatos, tratar valores ausentes e corrigir inconsistÃªncias.
- **Enriquecimento**: Calcular indicadores tÃ©cnicos com bibliotecas Python, como:
- **Pandas** para manipulaÃ§Ã£o de dados.
- **PySpark** para manipulaÃ§Ã£o de dados.

---

### 3. **Armazenamento**
- **Banco de Dados em Tempo Real**: Utilizar **Apache Cassandra** ou **PostgreSQL** com extensÃ£o **TimescaleDB**.
- **Data Lake**: Utilizar **MinIO** como alternativa open source ao armazenamento de dados brutos.
- **Particionamento**: Estruturar os dados por data e hora.

---

### 4. **VisualizaÃ§Ã£o e AnÃ¡lise**
- **Dashboards**: Construir painÃ©is interativos com **Apache Superset**.
- **Alertas**: Configurar notificaÃ§Ãµes automÃ¡ticas com **Prometheus Alertmanager**.
- **RelatÃ³rios**: Gerar relatÃ³rios com bibliotecas Python como **Matplotlib** e **Seaborn** para visualizaÃ§Ã£o personalizada.

---

## ğŸ› ï¸ Tecnologias e Ferramentas

| **Categoria**          | **Tecnologia/Ferramenta**                                   |
|-------------------------|------------------------------------------------------------|
| Linguagens de ProgramaÃ§Ã£o | Python (pandas, NumPy, Spark ), SQL                    |
| Frameworks de Streaming | Apache Kafka, Apache Spark Streaming, Apache Flink        |
| Bancos de Dados         | Apache Cassandra, PostgreSQL                |
| Ferramentas de VisualizaÃ§Ã£o | Apache Superset                              |
| Data Lake               | MinIO                                                     |
| Outras Ferramentas      | Prometheus, Airflow, Iceberg                                   |

---

## âš ï¸ Desafios e ConsideraÃ§Ãµes

- **LatÃªncia**: Garantir baixa latÃªncia em todo o pipeline.
- **Escalabilidade**: Configurar o sistema para suportar aumento no volume de dados.
- **Confiabilidade**: Implementar tolerÃ¢ncia a falhas com backups e replicaÃ§Ã£o.

---

## ğŸ“Š Resultados Esperados

- **Monitoramento ContÃ­nuo**: Dados financeiros em tempo real disponÃ­veis para anÃ¡lises.
- **VisualizaÃ§Ãµes Interativas**: Dashboards configurÃ¡veis para exibir tendÃªncias e padrÃµes.
- **Insights de Qualidade**: Dados limpos e enriquecidos para anÃ¡lises aprofundadas.

---

## ğŸš€ Como Executar

### PrÃ©-requisitos
1. Instale as dependÃªncias necessÃ¡rias listadas no `requirements.txt`.
2. Configure o Kafka e o Cassandra/PostgreSQL com os scripts fornecidos no repositÃ³rio.
3. Configure as variÃ¡veis de ambiente para chaves de API e parÃ¢metros de ingestÃ£o.
4. Execute o pipeline seguindo os comandos disponÃ­veis no diretÃ³rio principal.

---

## Estrutura

src/
â”œâ”€â”€ data/  # Dados brutos e processados
â”œâ”€â”€ notebooks/  # Notebooks Jupyter para anÃ¡lise exploratÃ³ria e prototipagem
â”œâ”€â”€ src/  # CÃ³digo fonte dos scripts Python
â”‚   â”œâ”€â”€ modules/  # MÃ³dulos reutilizÃ¡veis
â”‚   â”œâ”€â”€ pipelines/  # Scripts para execuÃ§Ã£o de pipelines
â”‚   â”œâ”€â”€ utils/  # FunÃ§Ãµes utilitÃ¡rias
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ sql/  # Scripts SQL
â”œâ”€â”€ tests/  # Testes unitÃ¡rios e de integraÃ§Ã£o
â”œâ”€â”€ airflow/  # DefiniÃ§Ãµes de DAGs no Airflow
â”œâ”€â”€ docs/  # DocumentaÃ§Ã£o
â””â”€â”€ requirements.txt  # DependÃªncias
infra/
â”œâ”€â”€ docker/  # Arquivos Dockerfile e docker-compose.yml
â””â”€â”€ kubernetes/  # Manifestos Kubernetes
